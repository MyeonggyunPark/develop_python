from bs4 import BeautifulSoup
import requests

# ê¸°ë³¸ URL / ê¸°ë³¸ user-agent
BASE_URL = "https://berlinstartupjobs.com/"
USER_HEADERS = {
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36"
}

# ì§êµ°ë³„/ìŠ¤íƒë³„ ê²€ìƒ‰ í‚¤ì›Œë“œ = ìµœì¢… ê²€ìƒ‰ í‚¤ì›Œë“œ
category_keywords = ["engineering", "design-ux"]
skill_keywords = ["python", "typescript", "javascript"]
all_keywords = category_keywords+skill_keywords

urls_list = []

# ì¹´í…Œê³ ë¦¬/ìŠ¤í‚¬ì— ë”°ë¼ URL ìƒì„± í•¨ìˆ˜
def get_url(keyword):
    if keyword in category_keywords:
        url = BASE_URL+keyword+"/"
    else:
        url = BASE_URL+"skill-areas/"+keyword+"/"
    urls_list.append(url)


# ë‹¨ì¼ íƒœê·¸ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜
def extract_text(tag):
    if isinstance(tag, list):
        if tag:
            return [t.text.strip() for t in tag]
        else:
            return "No Information"
    elif tag:
        return tag.text.strip()
    else:
        return "No Information"

# í•´ë‹¹ URLì˜ ì´ í˜ì´ì§€ ìˆ˜ í™•ì¸ í•¨ìˆ˜
def get_pages(url, headers):
    print(f"Seaching...")

    response = requests.get(url, headers=headers)

    if response.status_code == 200:
        print("ğŸŸ¢ Success get all page")
        soup = BeautifulSoup(response.text, "html.parser")
        page_list = soup.find_all("a", class_="page-numbers")

        return len(page_list)
    else:
        print(f"ğŸ”´ Erro-{response.status_code}")

# ê° URLì—ì„œ ê³µê³  ì •ë³´ ì¶”ì¶œ í•¨ìˆ˜
infos_list = []
def get_infos(url, headers):
    print(f"Scraping...")

    response = requests.get(url, headers=headers)

    if response.status_code == 200:
        print("ğŸŸ¢ Success get all infos")

        soup = BeautifulSoup(response.text, "html.parser")

        jobs_list = soup.find_all("li", class_="bjs-jlid")

        for job in jobs_list:

            title_tag = job.find("h4", class_="bjs-jlid__h")
            company_tag = job.find("a", class_="bjs-jlid__b")
            skills_tag = job.find_all("a", class_="bjs-bl bjs-bl-porcelain")
            description_tag = job.find("div", class_="bjs-jlid__description")

            title = extract_text(title_tag)
            company = extract_text(company_tag)
            required_skills = extract_text(skills_tag)
            description = extract_text(description_tag)
            title_link = title_tag.find("a")["href"]

            infos = {
                "company": company,
                "title": title,
                "skills": required_skills,
                "description": description,
                "link": title_link
            }
            infos_list.append(infos)

    else:
        print(f"ğŸ”´ Erro-{response.status_code}")
# ì •ë³´ ì¶œë ¥ í•¨ìˆ˜
def infos_print(info):
    print("\n======= [ğŸ“‘ INFO] ======")
    for k, v in info.items():
        if isinstance(v, list):
            print(f"{k}: {" / ".join(v)}")
        else:
            print(f"{k}: {v}")

# URL ëª©ë¡ ìƒì„±
for keyword in all_keywords:
    get_url(keyword)

# ì²« ë²ˆì§¸ ì¹´í…Œê³ ë¦¬ engineeringì˜ í˜ì´ì§€ ìˆ˜ë§Œí¼ ìˆœíšŒ
for page in range(get_pages(urls_list[0], USER_HEADERS)):
    url = f"{urls_list[0]}page/{page+1}"
    get_infos(url, USER_HEADERS)

# ê²°ê³¼ ì¶œë ¥
for info in infos_list:
    infos_print(info)

# skill-based URLë“¤ ì¶”ì¶œ
new_url_list = urls_list[2:]

for url in new_url_list:
    get_infos(url, USER_HEADERS)

# ê²°ê³¼ ì¶œë ¥
for info in infos_list:
    infos_print(info)




# í„°ë¯¸ë„ì—ì„œ í…ìŠ¤íŠ¸ ìƒ‰ìƒì„ ì ìš©í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from termcolor import colored


# ìœ„ì˜ í•¨ìˆ˜í˜• êµ¬í˜„ ì½”ë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ í´ë˜ìŠ¤ë¡œ êµ¬í˜„
class JobsScraper:
    def __init__(self):
        self.BASE_URL = "https://berlinstartupjobs.com/"
        self.USER_HEADERS = {
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36"
        }
        self.category_keywords = ["engineering", "design-ux"]
        self.skill_keywords = ["python", "typescript", "javascript"]
        self.all_keywords = self.category_keywords + self.skill_keywords
        self.urls_list = []
        self.infos_list = []

    def get_url(self, keyword):
        """ì¹´í…Œê³ ë¦¬/ìŠ¤í‚¬ì— ë”°ë¼ URL ìƒì„±"""

        if keyword in self.category_keywords:
            url = self.BASE_URL + keyword + "/"
        else:
            url = self.BASE_URL + "skill-areas/" + keyword + "/"
        self.urls_list.append(url)

    def extract_text(self, tag):
        """ë‹¨ì¼ íƒœê·¸ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""

        if isinstance(tag, list):
            return [t.text.strip() for t in tag] if tag else ["No Information"]
        elif tag:
            return tag.text.strip()
        else:
            return "No Information"

    def get_pages(self, url):
        """í•´ë‹¹ URLì˜ ì´ í˜ì´ì§€ ìˆ˜ í™•ì¸"""

        print(f"ğŸ“„ Checking pages for: {url}")
        response = requests.get(url, headers=self.USER_HEADERS)

        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            page_list = soup.find_all("a", class_="page-numbers")
            return len(page_list)
        else:
            print(f"âŒ Error fetching pages: {response.status_code}")
            return 0

    def get_infos(self, url):
        """ê° URLì—ì„œ ê³µê³  ì •ë³´ ì¶”ì¶œ"""

        print(f"ğŸ” Scraping: {url}")
        response = requests.get(url, headers=self.USER_HEADERS)

        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            jobs_list = soup.find_all("li", class_="bjs-jlid")

            for job in jobs_list:
                title_tag = job.find("h4", class_="bjs-jlid__h")
                company_tag = job.find("a", class_="bjs-jlid__b")
                skills_tag = job.find_all("a", class_="bjs-bl bjs-bl-porcelain")
                description_tag = job.find("div", class_="bjs-jlid__description")

                link_tag = title_tag.find("a") if title_tag else None
                title_link = (
                    link_tag["href"]
                    if link_tag and "href" in link_tag.attrs
                    else "No Link"
                )

                infos = {
                    "company": self.extract_text(company_tag),
                    "title": self.extract_text(title_tag),
                    "skills": self.extract_text(skills_tag),
                    "description": self.extract_text(description_tag),
                    "link": title_link,
                }

                self.infos_list.append(infos)
        else:
            print(f"âŒ Error fetching infos: {response.status_code}")

    def infos_print(self, info):
        """ì •ë³´ ì¶œë ¥ í•¨ìˆ˜"""
        print(colored("\n======= [ğŸ“‘ INFO] ======", "cyan"))
        for k, v in info.items():
            key_label = colored(f"{k}:", "yellow")
            if isinstance(v, list):
                print(f"{key_label} {' / '.join(v)}")
            else:
                print(f"{key_label} {v}")

    def run(self):
        """ì „ì²´ ì‹¤í–‰ í•¨ìˆ˜"""

        # URL ëª©ë¡ ìƒì„±
        for keyword in self.all_keywords:
            self.get_url(keyword)

        # ì²« ë²ˆì§¸ ì¹´í…Œê³ ë¦¬ engineeringì˜ í˜ì´ì§€ ìˆ˜ë§Œí¼ ìˆœíšŒ
        first_url = self.urls_list[0]
        total_pages = self.get_pages(first_url)

        for page in range(total_pages):
            paged_url = f"{first_url}page/{page + 1}"
            self.get_infos(paged_url)

        # skill-based URLë“¤ ì¶”ì¶œ
        for url in self.urls_list[2:]:
            self.get_infos(url)

        # ê²°ê³¼ ì¶œë ¥
        for info in self.infos_list:
            self.infos_print(info)


if __name__ == "__main__":
    scraper = JobsScraper()
    scraper.run()
