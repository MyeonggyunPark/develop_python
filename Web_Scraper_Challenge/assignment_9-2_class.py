import time
import csv
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup


class WantedScraper:
    """Wanted ì±„ìš©ê³µê³  ìŠ¤í¬ë˜í¼ í´ë˜ìŠ¤"""

    def __init__(self, keyword, scroll_times, headless=False):
        self.keyword = keyword
        self.scroll_times = scroll_times
        self.headless = headless
        self.filename = f"{keyword}.csv"
        self.jobs = []

    def run(self):
        """ì „ì²´ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰"""
        html = self.get_page_source()
        self.jobs = self.parse_jobs(html)
        self.save_to_csv(self.jobs, self.filename)
        print("ğŸ¯ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ")

    def get_page_source(self):
        """Playwrightë¡œ í˜ì´ì§€ ì—´ê³  HTML ì¶”ì¶œ"""
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=self.headless)
            page = browser.new_page()

            search_url = (
                f"https://www.wanted.co.kr/search?query={self.keyword}&tab=position"
            )
            page.goto(search_url)
            page.wait_for_load_state("load")

            for _ in range(self.scroll_times):
                page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                time.sleep(2)

            content = page.content()
            browser.close()

        print("âœ… HTML ì½”ë“œ ì¶”ì¶œ ì™„ë£Œ")
        return content

    def parse_jobs(self, html):
        """BeautifulSoupìœ¼ë¡œ HTML íŒŒì‹±"""
        soup = BeautifulSoup(html, "html.parser")
        jobs_list = soup.find_all("div", class_="JobCard_container__zQcZs")
        jobs_db = []

        for job in jobs_list:
            title = job.find("strong", class_="JobCard_title___kfvj").text.strip()
            company_name = job.find(
                "span",
                class_="CompanyNameWithLocationPeriod_CompanyNameWithLocationPeriod__company__ByVLu",
            ).text.strip()
            required_experience = job.find(
                "span",
                class_="CompanyNameWithLocationPeriod_CompanyNameWithLocationPeriod__location__4_w0l",
            ).text.strip()
            link = f"https://www.wanted.co.kr/{job.find('a')['href']}"

            jobs_db.append(
                {
                    "title": title,
                    "company_name": company_name,
                    "experience": required_experience,
                    "link": link,
                }
            )

        print(f"âœ… {len(jobs_db)}ê°œ ê³µê³  íŒŒì‹± ì™„ë£Œ")
        return jobs_db

    def save_to_csv(self, jobs, filename):
        """CSV íŒŒì¼ë¡œ ì €ì¥"""
        with open(filename, "w", encoding="utf-8", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=jobs[0].keys())
            writer.writeheader()
            writer.writerows(jobs)

        print(f"âœ… CSV ì €ì¥ ì™„ë£Œ: {filename}")


if __name__ == "__main__":
    
    search_keywords = ["Python", "JavaScript", "Java"]

    for keyword in search_keywords:
        scraper = WantedScraper(keyword, 4, headless=False)
        scraper.run()
